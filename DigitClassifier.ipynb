{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yiqichen\\UB\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import necessary python packages\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\" The logistic softmax function.\"\"\"\n",
    "    \n",
    "    return np.exp(z)/sum(np.exp(z))\n",
    "\n",
    "def EMS_logistic(weight,x_input,y_input,label_input,lamb):\n",
    "    \"\"\" This function returns the EMS for categorical cross entropy and accuracy for logistic regression.\n",
    "    weight: the weight matrix for logistic regression.\n",
    "    x_input: the input data matrix.\n",
    "    y_input: the traget matrix.\n",
    "    label_input: the label vector.\n",
    "    lamb: regularization factor.\"\"\"\n",
    "    \n",
    "    y_pred = softmax(weight.dot(x_input.T)).T\n",
    "    entropy = 0\n",
    "    for i in range(len(label_input)):\n",
    "        entropy += -np.dot(y_input[i],np.log(y_pred[i]))\n",
    "    entropy = np.sqrt(2*(entropy+np.sqrt(np.sum(weight**2))*lamb) / len(label_input))\n",
    "    count = 0 \n",
    "    for i in range(len(label_input)):\n",
    "        if np.argmax(y_pred[i]) == label_input[i]:\n",
    "            count += 1\n",
    "    return entropy, count/len(label_input)\n",
    "\n",
    "    \n",
    "def logistic_regression(x_train, y_train, lamb, lr, epoch, silent=True):\n",
    "    \"\"\" This function performs the stochastic gradient descent solution for weight w on logistic regression.\n",
    "    Inputs:\n",
    "    x_train: Training data.\n",
    "    y_train: Trainig label.\n",
    "    lamb, lr: regularization term and learning rate, respectively.\n",
    "    epoch: number of epochs ro run.\n",
    "    silent: whether to output the training progress.\"\"\"\n",
    "    \n",
    "    w = np.random.uniform(low=0.0, high=1.0, size=(y_train.shape[1], x_train.shape[1]))\n",
    "    for j in range(epoch):\n",
    "        sum_error = 0\n",
    "        rand_index = np.random.permutation(list(range(y_train.shape[0])))\n",
    "        for i in rand_index:        \n",
    "            E_del = -np.outer((y_train[i]-softmax(w.dot(x_train[i].T))),x_train[i].T)+lamb*w\n",
    "            sum_error += np.sum(E_del**2) / y_train.shape[0]\n",
    "            w = w - lr*E_del\n",
    "        if not silent:\n",
    "            print('>epoch={}, lrate={}, error={}'.format(j+1, lr, sum_error))\n",
    "    \n",
    "    return w  \n",
    "\n",
    "def select_params_logistic(lamb_list,lr_list,x_train,y_train,label_train,x_val,y_val,label_val,epoch):\n",
    "    \"\"\" This function compares model performances from different parameters based on the validation accuracy for logistic regression.\n",
    "    lamb_list: list of regularization factors to search.\n",
    "    lr_list: list of learning rates to search.\n",
    "    x_train, y_train, label_train: training data, training traget matrix and training labels. \n",
    "    x_val,y_val,label_val: validation data, validation target matrix and validation labels.\n",
    "    epoch: number of epochs to run.\"\"\"\n",
    "    \n",
    "    max_acc=0\n",
    "    lamb_best = lamb_list[0]\n",
    "    lr_best = lr_list[0]\n",
    "    acc_all = np.zeros((lr_list.shape[0],lamb_list.shape[0]))\n",
    "    for i in range(len(lr_list)):\n",
    "        for j in range(len(lamb_list)):\n",
    "            w = logistic_regression(x_train, y_train, lamb_list[j], lr_list[i], epoch)\n",
    "            E_val,acc_val = EMS_logistic(w,x_val,y_val,label_val,lamb_list[j])\n",
    "            acc_all[i,j] = acc_val\n",
    "            if acc_val > max_acc:\n",
    "                max_acc = acc_val\n",
    "                lamb_best = lamb_list[j]\n",
    "                lr_best = lr_list[i]\n",
    "    return lamb_best, lr_best, acc_all  \n",
    "\n",
    "def get_cnn_model(filters,drop_out):\n",
    "    \"\"\"\" This function returns a convolutional neural networks model which has one convolutional layer and one standard NN layer.\n",
    "    filters: Number of filters.\n",
    "    drop_out: Dropout rate.\"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters = filters, kernel_size=(3,3), activation='relu',\n",
    "                    input_shape=(28,28,1)))\n",
    "    model.add(BatchNormalization())  #Can make training faster\n",
    "    model.add(Conv2D(filters = filters, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(strides=(2,2)))\n",
    "    model.add(Dropout(drop_out)) #prevent overfitting\n",
    "    \n",
    "    model.add(Flatten()) #Compress output into one vector\n",
    "    model.add(Dense(512, activation='relu')) #First hidden layer in NN\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(10, activation='softmax')) #final output\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_best_params_cnn(x_train,y_train,x_val,y_val,filter_list,do_list):\n",
    "    \"\"\" This function searches for the best combination of the two hyperparameters in cnn \n",
    "    filter_list: number of filters to search from.\n",
    "    do_list: dropout rates to search from.\"\"\"\n",
    "    \n",
    "    # Specify key parameters\n",
    "    num_epochs = 5\n",
    "    model_batch_size = 200\n",
    "    \n",
    "    # First find the best number of filters        \n",
    "    val_acc_filter = []\n",
    "    for fil in filter_list:\n",
    "        model = get_cnn_model(fil,do_list[2])\n",
    "        \n",
    "        history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            epochs=num_epochs,\n",
    "                            batch_size=model_batch_size,\n",
    "                            verbose=1)\n",
    "        print('Number of filter {} is tested.'.format(fil))\n",
    "        del model\n",
    "        df = pd.DataFrame(history.history)\n",
    "        val_acc_filter.append(np.max(df['val_acc']))\n",
    "    index = np.argmax(val_acc_filter)\n",
    "    filter_best = filter_list[index]\n",
    "    print('Best number of filter for this model is {}'.format(filter_best))\n",
    "    \n",
    "    #Now choose the best dropout rate.\n",
    "    val_acc_do = []\n",
    "    for drop_out in do_list:\n",
    "        model = get_cnn_model(filter_list[2],drop_out)    \n",
    "        \n",
    "        history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            epochs=num_epochs,\n",
    "                            batch_size=model_batch_size,\n",
    "                            verbose=1)\n",
    "        print('Dropout rate {} is tested.'.format(drop_out))\n",
    "        del model\n",
    "        df = pd.DataFrame(history.history)\n",
    "        val_acc_do.append(np.max(df['val_acc']))\n",
    "    index = np.argmax(val_acc_do)\n",
    "    do_best = do_list[index]\n",
    "    print('Best drop out for this model is {}'.format(do_best))\n",
    "    \n",
    "    return filter_best, do_best, val_acc_filter, val_acc_do \n",
    "\n",
    "def get_dnn_model(drop_out,nodes):\n",
    "    \"\"\"\" This function returns a two layer deep neural networks model.\n",
    "    drop_out: Dropout rate.\n",
    "    nodes: number of nodes in hidden layers.\"\"\"\n",
    "    \n",
    "    first_dense_layer_nodes = nodes\n",
    "    second_dense_layer_nodes = nodes\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=784))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_out))\n",
    "\n",
    "    model.add(Dense(second_dense_layer_nodes))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_out))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))        \n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model   \n",
    "\n",
    "def get_best_params_dnn(x_train,y_train,x_val,y_val,nodes_list,do_list):\n",
    "    \"\"\" This function searches for the best combination of the two hyperparameters in cnn \n",
    "    x_train, y_train: training data and training target matrix.\n",
    "    x_val, y_val: validation data and validation target matrix.\n",
    "    nodes_list: number of nodes in hidden layers to search from.\n",
    "    do_list: dropout rates to search from.\"\"\"\n",
    "    \n",
    "    # Specify some key parameters\n",
    "    num_epochs = 100\n",
    "    model_batch_size = 100\n",
    "    tb_batch_size = 32\n",
    "    early_patience = 20\n",
    "    \n",
    "    tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "    earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "    \n",
    "    # First find the best number of nodes        \n",
    "    val_acc_nodes = []\n",
    "    for nodes in nodes_list:\n",
    "        model = get_dnn_model(do_list[2],nodes)\n",
    "        \n",
    "        history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            epochs=num_epochs,\n",
    "                            batch_size=model_batch_size,\n",
    "                            callbacks = [tensorboard_cb,earlystopping_cb],\n",
    "                            verbose=0)\n",
    "        print('nodes {} is tested.'.format(nodes))\n",
    "        del model\n",
    "        df = pd.DataFrame(history.history)\n",
    "        val_acc_nodes.append(np.max(df['val_acc']))\n",
    "    index = np.argmax(val_acc_nodes)\n",
    "    nodes_best = nodes_list[index]\n",
    "    print('Best number of nodes for this model is {}'.format(nodes_best))\n",
    "    \n",
    "    #Now choose the best dropout rate.\n",
    "    val_acc_do = []\n",
    "    for drop_out in do_list:\n",
    "        model = get_dnn_model(drop_out,nodes_best)    \n",
    "        \n",
    "        history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            epochs=num_epochs,\n",
    "                            batch_size=model_batch_size,\n",
    "                            callbacks = [tensorboard_cb,earlystopping_cb],\n",
    "                            verbose=0)\n",
    "        print('Dropout rate {} is tested.'.format(drop_out))\n",
    "        del model\n",
    "        df = pd.DataFrame(history.history)\n",
    "        val_acc_do.append(np.max(df['val_acc']))\n",
    "    index = np.argmax(val_acc_do)\n",
    "    do_best = do_list[index]\n",
    "    print('Best drop out for this model is {}'.format(do_best))\n",
    "      \n",
    "    \n",
    "    return do_best, nodes_best,  val_acc_nodes, val_acc_do \n",
    "    \n",
    "def accuracy_metrics(model,x_input,y_input):\n",
    "    \"\"\" This function computes accuracies from machine learning model prediction.\n",
    "    model: the model to make predictions.\n",
    "    x_input: the test data.\n",
    "    y_input: the test labels.\"\"\"\n",
    "    y = model.predict(x_input)\n",
    "    y_pred = [np.argmax(a) for a in y]\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    acc = accuracy_score(y_input,y_pred)\n",
    "    return acc    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read USPS dataset\n",
    "x_usps = []\n",
    "label_usps = []\n",
    "for j in range(10):\n",
    "    train_files = [f for f in listdir('USPSdata/Numerals/'+str(j)+'/') if 'png' in f]\n",
    "    for f in train_files:\n",
    "        img = Image.open('USPSdata/Numerals/'+str(j)+'/'+f).convert('L').resize((28,28))\n",
    "        arr = list(img.getdata())\n",
    "        x_usps.append(arr)\n",
    "        label_usps.append(j)\n",
    "x_usps = np.array(x_usps)\n",
    "label_usps = np.array(label_usps)\n",
    "\n",
    "# Shuffle the dataset\n",
    "indices = np.random.permutation(x_usps.shape[0])\n",
    "x_usps_test = x_usps[indices,:]\n",
    "label_usps_test = label_usps[indices]\n",
    "# Normalize the dataset\n",
    "x_usps_test = 1.0-x_usps_test/255.0\n",
    "\n",
    "# Read MNIST data\n",
    "filename = 'mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "x_train, x_val, x_test = training_data[0], validation_data[0], test_data[0]\n",
    "label_train, label_val, label_test = training_data[1], validation_data[1], test_data[1]\n",
    "\n",
    "# One hot encoding for all the labels\n",
    "y_train = np_utils.to_categorical(label_train).astype(np.float64)\n",
    "y_val = np_utils.to_categorical(label_val).astype(np.float64)\n",
    "y_test = np_utils.to_categorical(label_test).astype(np.float64)\n",
    "y_usps_test = np_utils.to_categorical(label_usps_test).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five machine learning models are used to train the datasets. A grid search technique was applied in all models to find the optimum parameters. Because of the large size of the training set, it takes a very long time to perform the grid search. So the process was run once and then committed out. The best parameters were then hrad coded based on the grid search results. For the Gaussian support vector machines models, one single run takes 4-5 hrs, so it is impossible to run the grid search on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    # Finding the best hyperparameters for logistic regression\n",
    "#    lamb_list = [0.005, 0.01, 0.04, 0.07]\n",
    "#    lr_list = [0.0005, 0.001, 0.005, 0.01] \n",
    "#    lamb_best, lr_best, Eval_all = select_params_logistic(np.array(lamb_list),np.array(lr_list),x_train,y_train,label_train,x_val,y_val,label_val,20)\n",
    "#    lr_index = lr_list.index(lr_best)\n",
    "#    lamb_index = lamb_list.index(lamb_best)\n",
    "#    plt.figure()\n",
    "#    plt.plot(lamb_list, Eval_all[lr_index,:],'o')\n",
    "#    plt.xlabel(r'$\\lambda$', fontsize=20)\n",
    "#    plt.ylabel('Validation Accuracy', fontsize=20)\n",
    "#    plt.title(r'Logistic Regression', fontsize=20)\n",
    "#    plt.figure()\n",
    "#    plt.plot(lr_list, Eval_all[:,lamb_index],'o')\n",
    "#    plt.xlabel('Learning Rate', fontsize=20)\n",
    "#    plt.ylabel('Validation Accuracy', fontsize=20)\n",
    "#    plt.title(r'Logistic Regression ', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "-------Logistic Regression Results-------\n",
      "----------------------------------------------------\n",
      "Best parameters: \n",
      "Lambda = 0.005 \n",
      "rate = 0.0005\n",
      "EMS Training: 0.8567894208340373, accuracy Training: 0.90588\n",
      "EMS Validation: 0.8262256118911976, accuracy Validation: 0.9119\n",
      "EMS Testing: 0.8362218267663069, accuracy Testing: 0.9087\n",
      "EMS USPS: 2.164904157896523, accuracy USPS: 0.36566828341417074\n"
     ]
    }
   ],
   "source": [
    "lamb_best = 0.005\n",
    "lr_best = 0.0005\n",
    "\n",
    "# Construct the optimum logistic regression model\n",
    "w = logistic_regression(x_train, y_train, lamb_best, lr_best, 20, silent=True)\n",
    "E_train, acc_train = EMS_logistic(w,x_train,y_train,label_train,lamb_best)\n",
    "E_val, acc_val = EMS_logistic(w,x_val,y_val,label_val,lamb_best)\n",
    "E_test, acc_test = EMS_logistic(w,x_test,y_test,label_test,lamb_best)\n",
    "E_usps_test, acc_usps_test = EMS_logistic(w,x_usps_test,y_usps_test,label_usps_test,lamb_best)\n",
    "\n",
    "print ('----------------------------------------------------')\n",
    "print (\"-------Logistic Regression Results-------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"Best parameters: \\nLambda = {} \\nrate = {}\".format(lamb_best,lr_best))\n",
    "print (\"EMS Training: {}, accuracy Training: {}\".format(E_train,acc_train))\n",
    "print (\"EMS Validation: {}, accuracy Validation: {}\".format(E_val,acc_val))\n",
    "print (\"EMS Testing: {}, accuracy Testing: {}\".format(E_test,acc_test))    \n",
    "print (\"EMS USPS: {}, accuracy USPS: {}\".format(E_usps_test,acc_usps_test))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion matrix by logistic regression model on MNIST test set:\n",
      "[[ 960    0    1    1    1    5    7    1    4    0]\n",
      " [   0 1106    2    3    1    2    4    1   16    0]\n",
      " [   9    7  898   13   17    3   16   16   45    8]\n",
      " [   4    1   22  894    1   38    4   13   23   10]\n",
      " [   1    5    4    1  916    0    9    1    8   37]\n",
      " [  11    5    1   30   10  768   15    9   35    8]\n",
      " [  12    3    4    0   11   16  905    1    6    0]\n",
      " [   2   17   24    4    9    0    0  930    3   39]\n",
      " [   6    9    5   19    9   31   13   14  854   14]\n",
      " [  11    9    4    9   44   11    0   20    6  895]]\n",
      "Confusion matrix by logistic regression model on USPS test set:\n",
      "[[ 626    3  243   54  227  126   76   48  173  424]\n",
      " [ 170  405   14  302  358   73   34  313  301   30]\n",
      " [ 196   29 1176  153   76   89   88   76   87   29]\n",
      " [  93    3  131 1234   24  299   15   64   91   46]\n",
      " [  54   83   36   47 1014   83   31  148  317  187]\n",
      " [ 169   23  128  159   46 1140  118   80  101   36]\n",
      " [ 275   16  378   98  115  295  738    9   52   24]\n",
      " [ 189  209  234  443   72  100   24  339  328   62]\n",
      " [ 235   35  123  205  120  642  110   44  413   73]\n",
      " [  37  180  121  428  162   82   14  404  378  194]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrices for both MNIST and USPS datasets\n",
    "y_pred_log = softmax(w.dot(x_test.T)).T\n",
    "label_pred_log = [np.argmax(y_pred_log[i]) for i in range(len(y_pred_log))]\n",
    "cf_log = confusion_matrix(label_test, label_pred_log)\n",
    "y_usps_pred_log = softmax(w.dot(x_usps_test.T)).T\n",
    "label_usps_pred_log = [np.argmax(y_usps_pred_log[i]) for i in range(len(y_usps_pred_log))]\n",
    "cf_usps_log = confusion_matrix(label_usps_test, label_usps_pred_log)\n",
    "print ('\\n')\n",
    "print ('Confusion matrix by logistic regression model on MNIST test set:')\n",
    "print (cf_log)\n",
    "print ('Confusion matrix by logistic regression model on USPS test set:')\n",
    "print (cf_usps_log)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vectors Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    # Finding the best regularization factor for linear vector machines\n",
    "#    C_factors = [1,2,5,10]\n",
    "#    # Linear Kernel\n",
    "#    #Find the best regularization factor to use  \n",
    "#    acc_val_linear_all = []\n",
    "#    for C in C_factors:\n",
    "#        svc_linear = SVC(kernel='linear', C=C)\n",
    "#        svc_linear.fit(x_train,label_train)\n",
    "#        acc_val_svc_linear = svc_linear.score(x_val,label_val)\n",
    "#        acc_val_linear_all.append(acc_val_svc_linear)\n",
    "#    plt.figure()\n",
    "#    plt.plot(C_factors, acc_val_linear_all, 'o')\n",
    "#    plt.xlabel('Regularization Factor', fontsize=20)\n",
    "#    plt.ylabel('Validation Accuracy', fontsize=20)\n",
    "#    plt.title(r'Linear SVC', fontsize=20)\n",
    "#    \n",
    "#    C_best_linear = C_factors[np.argmax(acc_val_linear_all)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------\n",
      "-------Support Vector Machine Results-------\n",
      "----------------------------------------------------\n",
      "Linear Kernel\n",
      "Best parameter: regularization factor = 1\n",
      "accuracy Training: 0.97246\n",
      "accuracy Validation: 0.9423\n",
      "accuracy Testing: 0.939\n",
      "accuracy USPS: 0.3272163608180409\n"
     ]
    }
   ],
   "source": [
    "C_best_linear = 1\n",
    "\n",
    "svc_linear = SVC(kernel='linear', C=C_best_linear)\n",
    "svc_linear.fit(x_train,label_train)\n",
    "acc_train_svc_linear, acc_val_svc_linear, acc_test_svc_linear = svc_linear.score(x_train,label_train), svc_linear.score(x_val,label_val), svc_linear.score(x_test,label_test)\n",
    "acc_usps_test_svc_linear = svc_linear.score(x_usps_test, label_usps_test)\n",
    "#y_pred_svc = svc.predict(x_test)\n",
    "\n",
    "print ('\\n')\n",
    "print ('----------------------------------------------------')\n",
    "print (\"-------Support Vector Machine Results-------\")\n",
    "print ('----------------------------------------------------')\n",
    "print ('Linear Kernel')\n",
    "print (\"Best parameter: regularization factor = {}\".format(C_best_linear))\n",
    "print (\"accuracy Training: {}\".format(acc_train_svc_linear))\n",
    "print (\"accuracy Validation: {}\".format(acc_val_svc_linear))\n",
    "print (\"accuracy Testing: {}\".format(acc_test_svc_linear))    \n",
    "print (\"accuracy USPS: {}\".format(acc_usps_test_svc_linear)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion matrix by linear svc model on MNIST test set:\n",
      "[[ 959    0    5    2    2    4    7    0    1    0]\n",
      " [   0 1121    3    3    0    1    2    1    4    0]\n",
      " [   6    8  968    9    3    2   11   10   13    2]\n",
      " [   5    2   17  944    4   13    1    8   13    3]\n",
      " [   2    1   10    1  943    0    4    2    2   17]\n",
      " [  13    4    2   39    5  792    9    1   22    5]\n",
      " [  10    3   11    1    5   14  911    2    1    0]\n",
      " [   1    8   20   10    6    1    0  961    3   18]\n",
      " [   8    4    9   25   11   27    6    5  871    8]\n",
      " [   7    6    2   13   32    4    0   18    7  920]]\n",
      "Confusion matrix by linear svc model on USPS test set:\n",
      "[[ 483    3  320   70  254  298   58  125   14  375]\n",
      " [  60  475  136  300  342  167   22  413   62   23]\n",
      " [ 176   91 1163  126   55  209   60   62   37   20]\n",
      " [  70   59  294  922   14  509    5   54   54   19]\n",
      " [  26   25  136   76  884  194    9  463   84  103]\n",
      " [  60   17  166  250   85 1203   36   50  104   29]\n",
      " [ 168   23  730   50  154  312  539   16    4    4]\n",
      " [  26   85  190  664   60  310   13  526   89   37]\n",
      " [ 104   21  272  391  136  736   78   58  185   19]\n",
      " [  15   50  163  513  165  108    7  623  192  164]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrices for both MNIST and USPS datasets\n",
    "label_pred_svc = svc_linear.predict(x_test)\n",
    "cf_svc = confusion_matrix(label_test, label_pred_svc)\n",
    "label_usps_pred_svc = svc_linear.predict(x_usps_test)\n",
    "cf_usps_svc = confusion_matrix(label_usps_test, label_usps_pred_svc)\n",
    "print ('\\n')\n",
    "print ('Confusion matrix by linear svc model on MNIST test set:')\n",
    "print (cf_svc)\n",
    "print ('Confusion matrix by linear svc model on USPS test set:')\n",
    "print (cf_usps_svc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##     Radial Basis Functions Kernel with gamma = 1\n",
    "#    svc_rbf1 = SVC(kernel='rbf', gamma=1)\n",
    "#    svc_rbf1.fit(x_train,label_train)\n",
    "#    acc_train_svc_rbf1, acc_val_svc_rbf1, acc_test_svc_rbf1 = svc_rbf1.score(x_train,label_train), svc_rbf1.score(x_val,label_val), svc_rbf1.score(x_test,label_test)\n",
    "#    acc_usps_test_svc_rbf1 = svc_rbf1.score(x_usps_test, label_usps_test)    \n",
    "#\n",
    "#    print ('\\n')    \n",
    "#    print ('----------------------------------------------------')    \n",
    "#    print ('Radial Basis Function Kernel')\n",
    "#    print ('gamma=1')\n",
    "#    print (\"accuracy Training: {}\".format(acc_train_svc_rbf1))\n",
    "#    print (\"accuracy Validation: {}\".format(acc_val_svc_rbf1))\n",
    "#    print (\"accuracy Testing: {}\".format(acc_test_svc_rbf1))    \n",
    "#    print (\"accuracy USPS: {}\".format(acc_usps_test_svc_rbf1))\n",
    "#    label_pred_svc_rbf1 = svc_rbf1.predict(x_test)\n",
    "#    cf_svc_rbf1 = confusion_matrix(label_test, label_pred_svc_rbf1)\n",
    "#    label_usps_pred_svc_rbf1 = svc_rbf1.predict(x_usps_test)\n",
    "#    cf_usps_svc_rbf1 = confusion_matrix(label_usps_test, label_usps_pred_svc_rbf1)\n",
    "#    print ('\\n')\n",
    "#    print ('Confusion matrix by rbf svc model (gamma=1.0) on MNIST test set:')\n",
    "#    print (cf_svc_rbf1)\n",
    "#    print ('Confusion matrix by rbf svc model (gamma=1.0) on USPS test set:')\n",
    "#    print (cf_usps_svc_rbf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##     Radial Basis Functions Kernel with gamma = auto\n",
    "    \n",
    "#    svc_rbf2 = SVC(kernel='rbf', gamma='auto')\n",
    "#    svc_rbf2.fit(x_train,label_train)\n",
    "#    acc_train_svc_rbf2, acc_val_svc_rbf2, acc_test_svc_rbf2 = svc_rbf2.score(x_train,label_train), svc_rbf2.score(x_val,label_val), svc_rbf2.score(x_test,label_test)\n",
    "#    acc_usps_test_svc_rbf2 = svc_rbf2.score(x_usps_test, label_usps_test)   \n",
    "#   \n",
    "#    print ('gamma=auto')\n",
    "#    print (\"accuracy Training: {}\".format(acc_train_svc_rbf2))\n",
    "#    print (\"accuracy Validation: {}\".format(acc_val_svc_rbf2))\n",
    "#    print (\"accuracy Testing: {}\".format(acc_test_svc_rbf2))    \n",
    "#    print (\"accuracy USPS: {}\".format(acc_usps_test_svc_rbf2))\n",
    "#    label_pred_svc_rbf2 = svc_rbf2.predict(x_test)\n",
    "#    cf_svc_rbf2 = confusion_matrix(label_test, label_pred_svc_rbf2)\n",
    "#    label_usps_pred_svc_rbf2 = svc_rbf2.predict(x_usps_test)\n",
    "#    cf_usps_svc_rbf2 = confusion_matrix(label_usps_test, label_usps_pred_svc_rbf2)\n",
    "#    print ('\\n')\n",
    "#    print ('Confusion matrix by rbf svc model (gamma=auto) on MNIST test set:')\n",
    "#    print (cf_svc_rbf2)\n",
    "#    print ('Confusion matrix by rbf svc model (gamma=auto) on USPS test set:')\n",
    "#    print (cf_usps_svc_rbf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    # Find the best number of trees to use\n",
    "#    n_trees = [10,25,50,100,150,200]\n",
    "#    acc_val_all = []\n",
    "#    for n in n_trees:\n",
    "#        rf = RandomForestClassifier(n_estimators=n)\n",
    "#        rf.fit(x_train,label_train)\n",
    "#        acc_val_rf = rf.score(x_val,label_val)    \n",
    "#        acc_val_all.append(acc_val_rf)\n",
    "#    plt.figure()\n",
    "#    plt.plot(n_trees, acc_val_all,'o')\n",
    "#    plt.xlabel('Number of Trees', fontsize=20)\n",
    "#    plt.ylabel('Validation Accuracy', fontsize=20)\n",
    "#    plt.title(r'Random Forest', fontsize=20)\n",
    "#    n_trees_best = n_trees[np.argmax(acc_val_all)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------\n",
      "-------Random Forests Results-------\n",
      "----------------------------------------------------\n",
      "Best parameters: \n",
      "number of trees = 200\n",
      "accuracy Training: 1.0\n",
      "accuracy Validation: 0.9733\n",
      "accuracy Testing: 0.9698\n",
      "accuracy USPS: 0.4136706835341767\n"
     ]
    }
   ],
   "source": [
    "n_trees_best = 200\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=n_trees_best)\n",
    "rf.fit(x_train,label_train)\n",
    "acc_train_rf, acc_val_rf, acc_test_rf = rf.score(x_train,label_train), rf.score(x_val,label_val), rf.score(x_test,label_test)\n",
    "acc_usps_test_rf = rf.score(x_usps_test, label_usps_test)\n",
    "#y_pred_svc = svc.predict(x_test)\n",
    "\n",
    "print ('\\n')\n",
    "print ('----------------------------------------------------')\n",
    "print (\"-------Random Forests Results-------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"Best parameters: \\nnumber of trees = {}\".format(n_trees_best))\n",
    "print (\"accuracy Training: {}\".format(acc_train_rf))\n",
    "print (\"accuracy Validation: {}\".format(acc_val_rf))\n",
    "print (\"accuracy Testing: {}\".format(acc_test_rf))    \n",
    "print (\"accuracy USPS: {}\".format(acc_usps_test_rf)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion matrix by random forest model on MNIST test set:\n",
      "[[ 970    0    2    0    0    2    3    1    2    0]\n",
      " [   0 1122    3    3    0    1    3    0    2    1]\n",
      " [   6    0 1000    6    2    0    3    9    6    0]\n",
      " [   0    0   10  974    0    6    0    9    8    3]\n",
      " [   1    0    1    0  954    0    5    0    2   19]\n",
      " [   2    0    0   13    3  859    5    2    6    2]\n",
      " [   8    3    1    0    2    2  939    0    3    0]\n",
      " [   0    2   18    2    0    0    0  993    1   12]\n",
      " [   4    0    5   10    3    5    4    5  927   11]\n",
      " [   8    5    2    8   10    4    1    5    6  960]]\n",
      "Confusion matrix by random forest model on USPS test set:\n",
      "[[ 611   21  251   59  449  141   58  128    3  279]\n",
      " [   8  717   24  100   16  120   32  980    2    1]\n",
      " [  78   42 1220   73   56  196   12  313    6    3]\n",
      " [  30    8   66 1284   45  338    0  199    5   25]\n",
      " [   7  237   46   24 1104  144   13  377   29   19]\n",
      " [  74   47   68   75   13 1575    9  127    4    8]\n",
      " [ 281   50  232   18   99  378  810  119    3   10]\n",
      " [  29  355  391  225   31  244   28  689    1    7]\n",
      " [  39   54  156  210   93 1104   67  106  157   14]\n",
      " [  11  305  202  277  249  138    6  630   76  106]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrices for both MNIST and USPS datasets\n",
    "label_pred_rf = rf.predict(x_test)\n",
    "cf_rf = confusion_matrix(label_test, label_pred_rf)\n",
    "label_usps_pred_rf = rf.predict(x_usps_test)\n",
    "cf_usps_rf = confusion_matrix(label_usps_test, label_usps_pred_rf)\n",
    "print ('\\n')\n",
    "print ('Confusion matrix by random forest model on MNIST test set:')\n",
    "print (cf_rf)\n",
    "print ('Confusion matrix by random forest model on USPS test set:')\n",
    "print (cf_usps_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    # Find the best paramters for DNN\n",
    "#    nodes_list = [128,256,512,1024]\n",
    "#    do_list = [0.1,0.2,0.3,0.4]\n",
    "#    \n",
    "#    do_best, nodes_best, val_acc_nodes, val_acc_do = get_best_params_dnn(x_train,y_train,x_val,y_val,nodes_list,do_list)    \n",
    "#\n",
    "#    plt.figure()\n",
    "#    plt.plot(nodes_list, val_acc_nodes, 'o')\n",
    "#    plt.xlabel('Number of Nodes in Hidden Layers', fontsize=15)\n",
    "#    plt.ylabel('Validation Accuracy', fontsize=15)\n",
    "#    plt.xticks(fontsize=15)\n",
    "#    plt.yticks(fontsize=15)\n",
    "#    plt.title('Deep Neural Networks')\n",
    "#    plt.figure()\n",
    "#    plt.plot(do_list, val_acc_do, 'o')\n",
    "#    plt.xlabel('Drop Out Rate', fontsize=15)\n",
    "#    plt.ylabel('Validation Accuracy', fontsize=15)\n",
    "#    plt.xticks(fontsize=15)\n",
    "#    plt.yticks(fontsize=15)\n",
    "#    plt.title('Deep Neural Networks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 0.2425 - acc: 0.9259 - val_loss: 0.1120 - val_acc: 0.9673\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 18s 352us/step - loss: 0.1100 - acc: 0.9654 - val_loss: 0.0860 - val_acc: 0.9752\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 18s 353us/step - loss: 0.0809 - acc: 0.9743 - val_loss: 0.0815 - val_acc: 0.9749\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 18s 352us/step - loss: 0.0620 - acc: 0.9797 - val_loss: 0.0725 - val_acc: 0.9790\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 18s 358us/step - loss: 0.0540 - acc: 0.9824 - val_loss: 0.0754 - val_acc: 0.9779\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 18s 367us/step - loss: 0.0479 - acc: 0.9842 - val_loss: 0.0754 - val_acc: 0.9791\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 18s 370us/step - loss: 0.0401 - acc: 0.9867 - val_loss: 0.0820 - val_acc: 0.9790\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 18s 368us/step - loss: 0.0400 - acc: 0.9873 - val_loss: 0.0764 - val_acc: 0.9807\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 19s 379us/step - loss: 0.0376 - acc: 0.9882 - val_loss: 0.0901 - val_acc: 0.9765\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 0.0323 - acc: 0.9898 - val_loss: 0.0875 - val_acc: 0.9797\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 0.0330 - acc: 0.9893 - val_loss: 0.0738 - val_acc: 0.9825\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 18s 367us/step - loss: 0.0302 - acc: 0.9901 - val_loss: 0.0813 - val_acc: 0.9813\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 18s 365us/step - loss: 0.0320 - acc: 0.9903 - val_loss: 0.0787 - val_acc: 0.9813\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 18s 366us/step - loss: 0.0257 - acc: 0.9917 - val_loss: 0.0919 - val_acc: 0.9804\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 18s 368us/step - loss: 0.0257 - acc: 0.9922 - val_loss: 0.0922 - val_acc: 0.9808\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 18s 366us/step - loss: 0.0245 - acc: 0.9927 - val_loss: 0.0844 - val_acc: 0.9811\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 18s 366us/step - loss: 0.0262 - acc: 0.9914 - val_loss: 0.0915 - val_acc: 0.9826\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 18s 366us/step - loss: 0.0238 - acc: 0.9926 - val_loss: 0.0919 - val_acc: 0.9814\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 18s 368us/step - loss: 0.0251 - acc: 0.9922 - val_loss: 0.0995 - val_acc: 0.9811\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 18s 366us/step - loss: 0.0261 - acc: 0.9923 - val_loss: 0.1021 - val_acc: 0.9809\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 18s 368us/step - loss: 0.0244 - acc: 0.9935 - val_loss: 0.0954 - val_acc: 0.9822\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 18s 367us/step - loss: 0.0156 - acc: 0.9951 - val_loss: 0.0952 - val_acc: 0.9822\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 18s 367us/step - loss: 0.0205 - acc: 0.9937 - val_loss: 0.0974 - val_acc: 0.9827\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 18s 366us/step - loss: 0.0197 - acc: 0.9942 - val_loss: 0.1281 - val_acc: 0.9794\n",
      "Epoch 00024: early stopping\n",
      "\n",
      "\n",
      "----------------------------------------------------\n",
      "-------Deep Neural Networks Results-------\n",
      "----------------------------------------------------\n",
      "Best parameters: \n",
      "nodes = 1024 \n",
      "dropout rate = 0.3\n",
      "accuracy Training: 0.99672\n",
      "accuracy Validation: 0.9794\n",
      "accuracy Testing: 0.9787\n",
      "accuracy USPS: 0.5296764838241912\n"
     ]
    }
   ],
   "source": [
    "do_best = 0.3\n",
    "nodes_best = 1024\n",
    "\n",
    "dnn_best = get_dnn_model(do_best, nodes_best)\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= 32, write_graph= True)\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=20, mode='min')\n",
    "\n",
    "history = dnn_best.fit(x_train,\n",
    "                    y_train,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    callbacks = [tensorboard_cb,earlystopping_cb],\n",
    "                    verbose=1)    \n",
    "df = pd.DataFrame(history.history)\n",
    "df.plot(subplots=True, grid=True, figsize=(10,15))\n",
    "plt.title('Deep Neural Networks Optimum Performance')\n",
    "\n",
    "acc_train_dnn, acc_val_dnn, acc_test_dnn = accuracy_metrics(dnn_best, x_train,label_train), accuracy_metrics(dnn_best, x_val,label_val), accuracy_metrics(dnn_best, x_test,label_test)\n",
    "acc_usps_test_dnn = accuracy_metrics(dnn_best, x_usps_test,label_usps_test)\n",
    "\n",
    "print ('\\n')\n",
    "print ('----------------------------------------------------')\n",
    "print (\"-------Deep Neural Networks Results-------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"Best parameters: \\nnodes = {} \\ndropout rate = {}\".format(nodes_best,do_best))\n",
    "print (\"accuracy Training: {}\".format(acc_train_dnn))\n",
    "print (\"accuracy Validation: {}\".format(acc_val_dnn))\n",
    "print (\"accuracy Testing: {}\".format(acc_test_dnn))    \n",
    "print (\"accuracy USPS: {}\".format(acc_usps_test_dnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion matrix by DNN model on MNIST test set:\n",
      "[[ 973    1    0    0    0    0    4    1    1    0]\n",
      " [   0 1129    2    0    0    0    2    0    2    0]\n",
      " [   4    1 1001   15    1    0    0    5    5    0]\n",
      " [   0    0    1 1001    0    1    0    5    2    0]\n",
      " [   1    1    5    0  952    0    7    3    1   12]\n",
      " [   2    0    0   10    1  869    3    0    6    1]\n",
      " [   2    3    0    0    1    3  946    0    3    0]\n",
      " [   2    3    7    5    1    0    0 1008    2    0]\n",
      " [   3    1    2    8    1    3    0    4  951    1]\n",
      " [   1    4    0    9   10    7    1   13    7  957]]\n",
      "Confusion matrix by DNN model on USPS test set:\n",
      "[[ 732    0   91   96  187   53   97   50  137  557]\n",
      " [  35  773   45  124  425   39   64  391   65   39]\n",
      " [  77   18 1622  102   25   46   36   44   26    3]\n",
      " [  14    9  141 1559    6  215    3   18   34    1]\n",
      " [   3   78   27   33 1250   52   17  294  215   31]\n",
      " [  64    6   20  115   11 1530   31   44  169   10]\n",
      " [  65   15  209   22   42   89 1506    5   41    6]\n",
      " [  29  161  403  464   65   34   20  635  185    4]\n",
      " [ 158   18  168  361   91  202   90   91  800   21]\n",
      " [   7   72   91  431  171   38    2  619  383  186]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrices for both MNIST and USPS datasets\n",
    "y_pred_dnn = dnn_best.predict(x_test)\n",
    "label_pred_dnn = [np.argmax(y_pred_dnn[i]) for i in range(len(y_pred_dnn))]\n",
    "cf_dnn = confusion_matrix(label_test, label_pred_dnn)\n",
    "y_usps_pred_dnn = dnn_best.predict(x_usps_test)\n",
    "label_usps_pred_dnn = [np.argmax(y_usps_pred_dnn[i]) for i in range(len(y_usps_pred_dnn))]\n",
    "cf_usps_dnn = confusion_matrix(label_usps_test, label_usps_pred_dnn)\n",
    "print ('\\n')\n",
    "print ('Confusion matrix by DNN model on MNIST test set:')\n",
    "print (cf_dnn)\n",
    "print ('Confusion matrix by DNN model on USPS test set:')\n",
    "print (cf_usps_dnn)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reszie the x datasets\n",
    "x_train_cnn = x_train.reshape(-1, 28, 28, 1)\n",
    "x_val_cnn = x_val.reshape(-1,28,28,1)\n",
    "x_test_cnn = x_test.reshape(-1,28,28,1)\n",
    "x_usps_test_cnn = x_usps_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    # Find the best parameters for CNN\n",
    "#    filter_list = [8,16,24,32]\n",
    "#    do_list = [0.1,0.2,0.3,0.4]\n",
    "#    \n",
    "#    filter_best, do_best, val_acc_filter, val_acc_do = get_best_params_cnn(x_train,y_train,x_val,y_val,filter_list,do_list)\n",
    "#    \n",
    "#    plt.figure()\n",
    "#    plt.plot(filter_list, val_acc_filter, 'o')\n",
    "#    plt.xlabel('Number of Filters', fontsize=15)\n",
    "#    plt.ylabel('Validation Accuracy', fontsize=15)\n",
    "#    plt.xticks(fontsize=15)\n",
    "#    plt.yticks(fontsize=15)\n",
    "#    plt.title('Convolutional Neural Networks',fontsize=15)\n",
    "#    plt.figure()\n",
    "#    plt.plot(do_list, val_acc_do, 'o')\n",
    "#    plt.xlabel('Drop Out Rate', fontsize=15)\n",
    "#    plt.ylabel('Validation Accuracy', fontsize=15)\n",
    "#    plt.xticks(fontsize=15)\n",
    "#    plt.yticks(fontsize=15)\n",
    "#    plt.title('Convolutional Neural Networks',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 1286s 26ms/step - loss: 0.2358 - acc: 0.9330 - val_loss: 0.0797 - val_acc: 0.9743\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 1450s 29ms/step - loss: 0.0711 - acc: 0.9777 - val_loss: 0.0562 - val_acc: 0.9845\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 1508s 30ms/step - loss: 0.0502 - acc: 0.9842 - val_loss: 0.0520 - val_acc: 0.9866\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 1511s 30ms/step - loss: 0.0370 - acc: 0.9878 - val_loss: 0.0402 - val_acc: 0.9888\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 1506s 30ms/step - loss: 0.0319 - acc: 0.9892 - val_loss: 0.0396 - val_acc: 0.9890\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 1246s 25ms/step - loss: 0.0247 - acc: 0.9914 - val_loss: 0.0403 - val_acc: 0.9884\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 1538s 31ms/step - loss: 0.0236 - acc: 0.9922 - val_loss: 0.0425 - val_acc: 0.9887\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 1677s 34ms/step - loss: 0.0221 - acc: 0.9926 - val_loss: 0.0482 - val_acc: 0.9891\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 1673s 33ms/step - loss: 0.0214 - acc: 0.9928 - val_loss: 0.0424 - val_acc: 0.9900\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 1552s 31ms/step - loss: 0.0181 - acc: 0.9938 - val_loss: 0.0499 - val_acc: 0.9883\n",
      "\n",
      "\n",
      "----------------------------------------------------\n",
      "-------Convolutional Neural Networks Results-------\n",
      "----------------------------------------------------\n",
      "Best parameters: \n",
      "number of filters = 16 \n",
      "dropout rate = 0.3\n",
      "accuracy Training: 0.99808\n",
      "accuracy Validation: 0.9883\n",
      "accuracy Testing: 0.99\n",
      "accuracy USPS: 0.5852792639631982\n"
     ]
    }
   ],
   "source": [
    "filter_best = 16\n",
    "do_best_cnn = 0.3\n",
    "cnn_best = get_cnn_model(filter_best,do_best_cnn)\n",
    "history = cnn_best.fit(x_train_cnn,\n",
    "                y_train,\n",
    "                validation_data=(x_val_cnn,y_val),\n",
    "                epochs=10,\n",
    "                batch_size=200,\n",
    "                verbose=1)\n",
    "\n",
    "df = pd.DataFrame(history.history)\n",
    "df.plot(subplots=True, grid=True, figsize=(10,15))\n",
    "plt.title('Convolutional Neural Networks Optimum Performance')\n",
    "\n",
    "acc_train_cnn, acc_val_cnn, acc_test_cnn = accuracy_metrics(cnn_best,x_train_cnn,label_train), accuracy_metrics(cnn_best,x_val_cnn,label_val), accuracy_metrics(cnn_best,x_test_cnn,label_test)\n",
    "acc_usps_test_cnn = accuracy_metrics(cnn_best,x_usps_test_cnn,label_usps_test)\n",
    "\n",
    "print ('\\n')\n",
    "print ('----------------------------------------------------')\n",
    "print (\"-------Convolutional Neural Networks Results-------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"Best parameters: \\nnumber of filters = {} \\ndropout rate = {}\".format(filter_best,do_best))\n",
    "print (\"accuracy Training: {}\".format(acc_train_cnn))\n",
    "print (\"accuracy Validation: {}\".format(acc_val_cnn))\n",
    "print (\"accuracy Testing: {}\".format(acc_test_cnn))    \n",
    "print (\"accuracy USPS: {}\".format(acc_usps_test_cnn))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion matrix by CNN model on MNIST test set:\n",
      "[[ 977    0    1    0    0    0    0    1    1    0]\n",
      " [   0 1133    0    0    0    0    1    1    0    0]\n",
      " [   4    2 1020    1    1    0    1    3    0    0]\n",
      " [   0    0    0 1008    0    2    0    0    0    0]\n",
      " [   0    0    0    0  976    0    4    0    1    1]\n",
      " [   1    0    0    6    0  880    4    1    0    0]\n",
      " [   4    2    0    0    1    1  950    0    0    0]\n",
      " [   0    1    4    1    0    0    0 1021    1    0]\n",
      " [   1    0    2    4    0    0    2    1  961    3]\n",
      " [   2    4    0    3    8    5    0    9    4  974]]\n",
      "Confusion matrix by CNN model on USPS test set:\n",
      "[[ 812    2   45   36  193    5   20    9  116  762]\n",
      " [  51  932   55   21  492   18   38  324   66    3]\n",
      " [  56   33 1566   93   24   23   18   32  152    2]\n",
      " [   7    2   35 1665    1  225    5    3   54    3]\n",
      " [   4   13   47   29 1382   13   13  169  322    8]\n",
      " [  26    0    7  206    3 1650    4   12   30   62]\n",
      " [ 200    4  139   12   53   32 1472    2   75   11]\n",
      " [  43   61  287  618   19   17    7  806  140    2]\n",
      " [  45    2   36  384   39  275   14   37 1030  138]\n",
      " [   4   12  231  531  135    5    1  284  407  390]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrices for both MNIST and USPS datasets\n",
    "y_pred_cnn = cnn_best.predict(x_test_cnn)\n",
    "label_pred_cnn = [np.argmax(y_pred_cnn[i]) for i in range(len(y_pred_cnn))]\n",
    "cf_cnn = confusion_matrix(label_test, label_pred_cnn)\n",
    "y_usps_pred_cnn = cnn_best.predict(x_usps_test_cnn)\n",
    "label_usps_pred_cnn = [np.argmax(y_usps_pred_cnn[i]) for i in range(len(y_usps_pred_cnn))]\n",
    "cf_usps_cnn = confusion_matrix(label_usps_test, label_usps_pred_cnn)\n",
    "print ('\\n')\n",
    "print ('Confusion matrix by CNN model on MNIST test set:')\n",
    "print (cf_cnn)\n",
    "print ('Confusion matrix by CNN model on USPS test set:')\n",
    "print (cf_usps_cnn)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------\n",
      "-------Combined Model (Hard Voting) Results-------\n",
      "----------------------------------------------------\n",
      "accuracy Testing: 0.978\n",
      "accuracy USPS: 0.4920246012300615\n"
     ]
    }
   ],
   "source": [
    "# 1. Hard Majority Voting Method\n",
    "\n",
    "label_pred_combine = []\n",
    "for i in range(len(label_test)):\n",
    "    label_temp = [label_pred_log[i], label_pred_svc[i], label_pred_rf[i], label_pred_dnn[i], label_pred_cnn[i]]\n",
    "    label_pred_combine.append(max(set(label_temp), key=label_temp.count))\n",
    "label_usps_pred_combine = []\n",
    "for j in range(len(label_usps_test)):\n",
    "    label_temp = [label_usps_pred_log[j], label_usps_pred_svc[j], label_usps_pred_rf[j], label_usps_pred_dnn[j], label_usps_pred_cnn[j]]\n",
    "    label_usps_pred_combine.append(max(set(label_temp), key=label_temp.count))\n",
    "\n",
    "# Calculate the accuracy of the combined model on different datasets\n",
    "acc_test_combine, acc_usps_test_combine = accuracy_score(label_test,label_pred_combine), accuracy_score(label_usps_test,label_usps_pred_combine)\n",
    "print ('\\n')\n",
    "print ('----------------------------------------------------')\n",
    "print (\"-------Combined Model (Hard Voting) Results-------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"accuracy Testing: {}\".format(acc_test_combine))    \n",
    "print (\"accuracy USPS: {}\".format(acc_usps_test_combine))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion matrix by the combined model (hard voting) on MNIST test set:\n",
      "[[ 975    0    1    0    0    1    1    1    1    0]\n",
      " [   0 1127    3    1    0    0    2    0    2    0]\n",
      " [   4    0 1009    5    1    0    1    6    6    0]\n",
      " [   0    0    7  992    0    1    0    3    6    1]\n",
      " [   1    0    4    0  962    0    4    0    2    9]\n",
      " [   3    0    0   11    0  863    5    1    7    2]\n",
      " [   7    3    1    0    2    4  939    0    2    0]\n",
      " [   1    2   14    1    0    0    0 1001    2    7]\n",
      " [   4    0    2    6    4    4    1    3  946    4]\n",
      " [   9    5    0    8   10    0    0    7    4  966]]\n",
      "Confusion matrix by the combined model (hard voting) on USPS test set:\n",
      "[[ 761    2  190   50  250   75   32   29   84  527]\n",
      " [  55  744   29  198  332   84   28  457   67    6]\n",
      " [ 133   27 1568   66   32   74   19   48   26    6]\n",
      " [  42    9  125 1507    6  240    1   23   37   10]\n",
      " [  15   91   29   35 1254   66    7  222  232   49]\n",
      " [  79   16   67  143   14 1559    9   30   68   15]\n",
      " [ 235   23  335   30   77  195 1068    4   18   15]\n",
      " [  77  208  318  490   35  120   10  558  171   13]\n",
      " [ 157   17  121  324   79  578   44   46  603   31]\n",
      " [  18  144  141  449  142   47    2  479  360  218]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrices for both MNIST and USPS datasets\n",
    "cf_combine = confusion_matrix(label_test, label_pred_combine)\n",
    "cf_usps_combine = confusion_matrix(label_usps_test, label_usps_pred_combine)\n",
    "print ('\\n')\n",
    "print ('Confusion matrix by the combined model (hard voting) on MNIST test set:')\n",
    "print (cf_combine)\n",
    "print ('Confusion matrix by the combined model (hard voting) on USPS test set:')\n",
    "print (cf_usps_combine)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "30000/30000 [==============================] - 11s 364us/step - loss: 0.5238 - acc: 0.8217 - val_loss: 0.1085 - val_acc: 0.9677\n",
      "Epoch 2/100\n",
      "30000/30000 [==============================] - 11s 357us/step - loss: 0.1721 - acc: 0.9426 - val_loss: 0.0979 - val_acc: 0.9681\n",
      "Epoch 3/100\n",
      "30000/30000 [==============================] - 11s 360us/step - loss: 0.1017 - acc: 0.9674 - val_loss: 0.0791 - val_acc: 0.9757\n",
      "Epoch 4/100\n",
      "30000/30000 [==============================] - 11s 359us/step - loss: 0.0762 - acc: 0.9750 - val_loss: 0.0866 - val_acc: 0.9738\n",
      "Epoch 5/100\n",
      "30000/30000 [==============================] - 11s 360us/step - loss: 0.0612 - acc: 0.9797 - val_loss: 0.0844 - val_acc: 0.9786\n",
      "Epoch 6/100\n",
      "30000/30000 [==============================] - 11s 361us/step - loss: 0.0505 - acc: 0.9827 - val_loss: 0.0891 - val_acc: 0.9762\n",
      "Epoch 7/100\n",
      "30000/30000 [==============================] - 11s 361us/step - loss: 0.0501 - acc: 0.9832 - val_loss: 0.0900 - val_acc: 0.9795\n",
      "Epoch 8/100\n",
      "30000/30000 [==============================] - 11s 362us/step - loss: 0.0401 - acc: 0.9865 - val_loss: 0.1016 - val_acc: 0.9770\n",
      "Epoch 9/100\n",
      "30000/30000 [==============================] - 11s 362us/step - loss: 0.0454 - acc: 0.9852 - val_loss: 0.1021 - val_acc: 0.9770\n",
      "Epoch 10/100\n",
      "30000/30000 [==============================] - 11s 360us/step - loss: 0.0313 - acc: 0.9891 - val_loss: 0.0998 - val_acc: 0.9781\n",
      "Epoch 11/100\n",
      "30000/30000 [==============================] - 11s 362us/step - loss: 0.0424 - acc: 0.9858 - val_loss: 0.1045 - val_acc: 0.9767\n",
      "Epoch 12/100\n",
      "30000/30000 [==============================] - 11s 359us/step - loss: 0.0318 - acc: 0.9890 - val_loss: 0.1083 - val_acc: 0.9782\n",
      "Epoch 13/100\n",
      "30000/30000 [==============================] - 11s 361us/step - loss: 0.0315 - acc: 0.9897 - val_loss: 0.1197 - val_acc: 0.9759\n",
      "Epoch 14/100\n",
      "30000/30000 [==============================] - 11s 360us/step - loss: 0.0339 - acc: 0.9888 - val_loss: 0.1179 - val_acc: 0.9752\n",
      "Epoch 15/100\n",
      "30000/30000 [==============================] - 11s 362us/step - loss: 0.0305 - acc: 0.9898 - val_loss: 0.1071 - val_acc: 0.9781\n",
      "Epoch 16/100\n",
      "30000/30000 [==============================] - 11s 359us/step - loss: 0.0349 - acc: 0.9883 - val_loss: 0.1117 - val_acc: 0.9778\n",
      "Epoch 17/100\n",
      "30000/30000 [==============================] - 11s 363us/step - loss: 0.0274 - acc: 0.9916 - val_loss: 0.1156 - val_acc: 0.9792\n",
      "Epoch 18/100\n",
      "30000/30000 [==============================] - 11s 361us/step - loss: 0.0221 - acc: 0.9924 - val_loss: 0.1151 - val_acc: 0.9781\n",
      "Epoch 19/100\n",
      "30000/30000 [==============================] - 11s 359us/step - loss: 0.0262 - acc: 0.9919 - val_loss: 0.1048 - val_acc: 0.9789\n",
      "Epoch 20/100\n",
      "30000/30000 [==============================] - 11s 361us/step - loss: 0.0240 - acc: 0.9920 - val_loss: 0.1236 - val_acc: 0.9774\n",
      "Epoch 21/100\n",
      "30000/30000 [==============================] - 11s 361us/step - loss: 0.0305 - acc: 0.9909 - val_loss: 0.1154 - val_acc: 0.9799\n",
      "Epoch 22/100\n",
      "30000/30000 [==============================] - 11s 359us/step - loss: 0.0256 - acc: 0.9919 - val_loss: 0.1194 - val_acc: 0.9787\n",
      "Epoch 23/100\n",
      "30000/30000 [==============================] - 11s 363us/step - loss: 0.0234 - acc: 0.9929 - val_loss: 0.1390 - val_acc: 0.9771\n",
      "Epoch 00023: early stopping\n",
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "30000/30000 [==============================] - 808s 27ms/step - loss: 0.4958 - acc: 0.8481 - val_loss: 0.0592 - val_acc: 0.9827\n",
      "Epoch 2/5\n",
      "30000/30000 [==============================] - 807s 27ms/step - loss: 0.1223 - acc: 0.9589 - val_loss: 0.0459 - val_acc: 0.9868\n",
      "Epoch 3/5\n",
      "30000/30000 [==============================] - 811s 27ms/step - loss: 0.0687 - acc: 0.9764 - val_loss: 0.0483 - val_acc: 0.9855\n",
      "Epoch 4/5\n",
      "30000/30000 [==============================] - 807s 27ms/step - loss: 0.0475 - acc: 0.9841 - val_loss: 0.0431 - val_acc: 0.9889\n",
      "Epoch 5/5\n",
      "30000/30000 [==============================] - 830s 28ms/step - loss: 0.0361 - acc: 0.9872 - val_loss: 0.0427 - val_acc: 0.9877\n"
     ]
    }
   ],
   "source": [
    "# 2. Boosting Method\n",
    "\n",
    "def missclassified(label_true,label_pred):\n",
    "    miss_list = []\n",
    "    for i in range(len(label_true)):\n",
    "        if label_pred[i] != label_true[i]:\n",
    "            miss_list.append(i)\n",
    "    return miss_list\n",
    "\n",
    "\"\"\" Construct all five models to be combined with \"\"\"\n",
    "svc_linear1 = SVC(kernel='linear', C=C_best_linear) \n",
    "rf1 = RandomForestClassifier(n_estimators=n_trees_best)\n",
    "dnn1 = get_dnn_model(do_best, nodes_best)  \n",
    "cnn1 = get_cnn_model(filter_best, do_best_cnn)\n",
    "\n",
    "\"\"\"iteration 1: logistic regreesion. Select 60% of training set randomly with equal weights. Then train with logistic regression.\"\"\"\n",
    "index_list = list(range(x_train.shape[0]))\n",
    "list1 = random.choices(index_list,k=int(0.6*x_train.shape[0]))\n",
    "x_train1, y_train1 = x_train[list1,:], y_train[list1,:]\n",
    "w1 = logistic_regression(x_train1, y_train1, lamb_best, lr_best, 20, silent=True)\n",
    "y_pred_log1 = softmax(w1.dot(x_train.T)).T\n",
    "label_pred_log1 = [np.argmax(y_pred_log1[i]) for i in range(x_train.shape[0])]\n",
    "miss_list1 = missclassified(label_train,label_pred_log1)\n",
    "# Set the weights of missclassified samples 5 times higher\n",
    "index_list += miss_list1*4\n",
    "\n",
    "\"\"\"iteration 2: SVC. Select 60% of training set randomly with the new weights. Then train with linear SVC.\"\"\"\n",
    "list2 = random.choices(index_list,k=int(0.6*x_train.shape[0]))\n",
    "x_train2, label_train2 = x_train[list2,:], label_train[list2]\n",
    "svc_linear1.fit(x_train2, label_train2)\n",
    "label_pred_svc1 = svc_linear1.predict(x_train)\n",
    "miss_list2 = missclassified(label_train,label_pred_svc1)\n",
    "index_list += miss_list2*4\n",
    "\n",
    "\"\"\"iteration 3: Random Forest. Select 60% of training set randomly with the new weights. Then train with random forest.\"\"\"\n",
    "list3 = random.choices(index_list,k=int(0.6*x_train.shape[0]))\n",
    "x_train3, label_train3 = x_train[list3,:], label_train[list3]\n",
    "rf1.fit(x_train3, label_train3)\n",
    "label_pred_rf1 = rf1.predict(x_train)\n",
    "miss_list3 = missclassified(label_train,label_pred_rf1)\n",
    "index_list += miss_list3*4    \n",
    "\n",
    "\"\"\"iteration 4: DNN. Select 60% of training set randomly with the new weights. Then train with DNN.\"\"\"\n",
    "list4 = random.choices(index_list,k=int(0.6*x_train.shape[0]))\n",
    "x_train4, y_train4 = x_train[list4,:], y_train[list4,:]\n",
    "history = dnn1.fit(x_train4,\n",
    "                    y_train4,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    callbacks = [tensorboard_cb,earlystopping_cb],\n",
    "                    verbose=1) \n",
    "y_pred_dnn1 = dnn1.predict(x_train)\n",
    "label_pred_dnn1 = [np.argmax(y_pred_dnn1[i]) for i in range(len(y_pred_dnn1))]      \n",
    "miss_list4 = missclassified(label_train,label_pred_dnn1)\n",
    "index_list += miss_list4*4 \n",
    "\n",
    "\"\"\"iteration 5: CNN. Select 60% of training set randomly with the new weights. Then train with CNN.\"\"\"\n",
    "list5 = random.choices(index_list,k=int(0.6*x_train.shape[0]))\n",
    "x_train5, y_train5 = x_train_cnn[list5,:,:,:], y_train[list5,:]\n",
    "history = cnn1.fit(x_train5,\n",
    "            y_train5,\n",
    "            validation_data=(x_val_cnn,y_val),\n",
    "            epochs=5,\n",
    "            batch_size=200,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use hard voting to combine these newly trained models\n",
    "y_pred_log1 = softmax(w1.dot(x_test.T)).T\n",
    "label_pred_log1 = [np.argmax(y_pred_log1[i]) for i in range(x_test.shape[0])]\n",
    "y_usps_pred_log1 = softmax(w1.dot(x_usps_test.T)).T\n",
    "label_usps_pred_log1 = [np.argmax(y_usps_pred_log1[i]) for i in range(x_usps_test.shape[0])]\n",
    "\n",
    "label_pred_svc1 = svc_linear1.predict(x_test)\n",
    "label_pred_rf1 = rf1.predict(x_test)\n",
    "label_usps_pred_svc1 = svc_linear1.predict(x_usps_test)\n",
    "label_usps_pred_rf1 = rf1.predict(x_usps_test)\n",
    "\n",
    "y_pred_dnn1 = dnn1.predict(x_test)\n",
    "label_pred_dnn1 = [np.argmax(y_pred_dnn1[i]) for i in range(len(y_pred_dnn1))]    \n",
    "y_usps_pred_dnn1 = dnn1.predict(x_usps_test)\n",
    "label_usps_pred_dnn1 = [np.argmax(y_usps_pred_dnn1[i]) for i in range(len(y_usps_pred_dnn1))]\n",
    "\n",
    "y_pred_cnn1 = cnn1.predict(x_test_cnn)\n",
    "label_pred_cnn1 = [np.argmax(y_pred_cnn1[i]) for i in range(len(y_pred_cnn1))]    \n",
    "y_usps_pred_cnn1 = cnn1.predict(x_usps_test_cnn)\n",
    "label_usps_pred_cnn1 = [np.argmax(y_usps_pred_cnn1[i]) for i in range(len(y_usps_pred_cnn1))]\n",
    "\n",
    "label_pred_combine1 = []\n",
    "for i in range(len(label_test)):\n",
    "    label_temp = [label_pred_log1[i], label_pred_svc1[i], label_pred_rf1[i], label_pred_dnn1[i], label_pred_cnn1[i]]\n",
    "    label_pred_combine1.append(max(set(label_temp), key=label_temp.count))\n",
    "label_usps_pred_combine1 = []\n",
    "for j in range(len(label_usps_test)):\n",
    "    label_temp = [label_usps_pred_log1[j], label_usps_pred_svc1[j], label_usps_pred_rf1[j], label_usps_pred_dnn1[j], label_usps_pred_cnn1[j]]\n",
    "    label_usps_pred_combine1.append(max(set(label_temp), key=label_temp.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------\n",
      "-------Combined Model (Boosting) Results-------\n",
      "----------------------------------------------------\n",
      "accuracy Testing: 0.9806\n",
      "accuracy USPS: 0.4786239311965598\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the combined model on different datasets\n",
    "acc_test_combine1, acc_usps_test_combine1 = accuracy_score(label_test,label_pred_combine1), accuracy_score(label_usps_test,label_usps_pred_combine1)\n",
    "print ('\\n')\n",
    "print ('----------------------------------------------------')\n",
    "print (\"-------Combined Model (Boosting) Results-------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"accuracy Testing: {}\".format(acc_test_combine1))    \n",
    "print (\"accuracy USPS: {}\".format(acc_usps_test_combine1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Confusion matrix by the combined model (boosting) on MNIST test set:\n",
      "[[ 970    0    1    1    0    1    3    0    4    0]\n",
      " [   0 1129    3    0    0    0    1    0    2    0]\n",
      " [   6    0 1011    1    1    0    1    7    4    1]\n",
      " [   0    0    4  992    0    3    0    4    7    0]\n",
      " [   2    0    2    0  968    0    2    0    0    8]\n",
      " [   2    1    0   10    0  866    4    1    6    2]\n",
      " [   6    2    2    1    3    3  940    0    1    0]\n",
      " [   2    5   14    2    0    0    0  999    2    4]\n",
      " [   3    0    4    3    4    2    1    2  951    4]\n",
      " [   7    7    0    3    6    0    0    4    2  980]]\n",
      "Confusion matrix by the combined model (boosting) on USPS test set:\n",
      "[[ 696    8  282   40  259   75   31   27   51  531]\n",
      " [  94  662   87  197  344   83   20  430   67   16]\n",
      " [ 109   19 1621   49   26   94   14   31   25   11]\n",
      " [  46    7  127 1477    6  271    3   20   31   12]\n",
      " [  19  122   47   20 1287   88    5  232  122   58]\n",
      " [  70   18   79  141   20 1577   16   22   44   13]\n",
      " [ 238   28  449   29   95  213  896    8   30   14]\n",
      " [  61  210  321  418   35  147   11  669  114   14]\n",
      " [ 152   31  192  345   88  624   69   47  420   32]\n",
      " [  20  196  172  344  149   64    2  541  245  267]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrices for both MNIST and USPS datasets\n",
    "cf_combine1 = confusion_matrix(label_test, label_pred_combine1)\n",
    "cf_usps_combine1 = confusion_matrix(label_usps_test, label_usps_pred_combine1)\n",
    "print ('\\n')\n",
    "print ('Confusion matrix by the combined model (boosting) on MNIST test set:')\n",
    "print (cf_combine1)\n",
    "print ('Confusion matrix by the combined model (boosting) on USPS test set:')\n",
    "print (cf_usps_combine1)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
